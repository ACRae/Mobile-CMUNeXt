
Module: input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 3, 256, 256])
  * Quant Output shape: (1, 3, 256, 256), bit_width=8.0, fixed-point format=Q0.7, scale=6.103515625e-05, signed=True

Module: stem.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 3, 256, 256), bit_width=8.0, fixed-point format=Q0.7, scale=6.103515625e-05, signed=True
  * Quant Weight shape: (8, 3, 3, 3), bit_width=8.0, fixed-point format=Q7.0, scale=1.0, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q7.14, scale=6.103515625e-05, signed=True

Module: stem.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q7.14, scale=6.103515625e-05, signed=True

Module: stem.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder1.block.0.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q6.9, scale=0.001953125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=21.0, fixed-point format=Q11.9, scale=0.001953125, signed=True

Module: encoder1.block.0.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=21.0, fixed-point format=Q11.9, scale=0.001953125, signed=True

Module: encoder1.block.0.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.0.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q6.9, scale=0.001953125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q12.9, scale=0.001953125, signed=True

Module: encoder1.block.0.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q12.9, scale=0.001953125, signed=True

Module: encoder1.block.0.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.0.2 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q5.3, scale=0.125, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 32, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.0.3 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.0.4 (QuantReLU)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.0.5 (QuantConv2d)
  * Quant Input shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder1.block.0.6 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder1.block.0.7 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.1.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.1.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.1.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.1.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.1.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.1.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q5.10, scale=0.0009765625, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q11.10, scale=0.0009765625, signed=True

Module: encoder1.block.1.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q11.10, scale=0.0009765625, signed=True

Module: encoder1.block.1.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.1.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.1.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True

Module: encoder1.block.1.2 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q5.3, scale=0.125, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 32, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.1.3 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.1.4 (QuantReLU)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.1.5 (QuantConv2d)
  * Quant Input shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder1.block.1.6 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder1.block.1.7 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.2.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder1.block.2.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q10.11, scale=0.00048828125, signed=True

Module: encoder1.block.2.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q10.11, scale=0.00048828125, signed=True

Module: encoder1.block.2.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.2 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 32, 256, 256), bit_width=21.0, fixed-point format=Q8.12, scale=0.000244140625, signed=True

Module: encoder1.block.2.3 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=21.0, fixed-point format=Q8.12, scale=0.000244140625, signed=True

Module: encoder1.block.2.4 (QuantReLU)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.block.2.5 (QuantConv2d)
  * Quant Input shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder1.block.2.6 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder1.block.2.7 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder1.up.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 8, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder1.up.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder1.up.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q6.9, scale=0.001953125, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=21.0, fixed-point format=Q11.9, scale=0.001953125, signed=True

Module: encoder2.block.0.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=21.0, fixed-point format=Q11.9, scale=0.001953125, signed=True

Module: encoder2.block.0.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 128, 128), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 1, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=22.0, fixed-point format=Q10.11, scale=0.00048828125, signed=True

Module: encoder2.block.0.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=22.0, fixed-point format=Q10.11, scale=0.00048828125, signed=True

Module: encoder2.block.0.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.2 (QuantConv2d)
  * Quant Input shape: (1, 8, 128, 128), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 32, 128, 128), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder2.block.0.3 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 128, 128])
  * Quant Output shape: (1, 32, 128, 128), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: encoder2.block.0.4 (QuantReLU)
  * Input shape: torch.Size([1, 32, 128, 128])
  * Quant Output shape: (1, 32, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.block.0.5 (QuantConv2d)
  * Quant Input shape: (1, 32, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=22.0, fixed-point format=Q10.11, scale=0.00048828125, signed=True

Module: encoder2.block.0.6 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=22.0, fixed-point format=Q10.11, scale=0.00048828125, signed=True

Module: encoder2.block.0.7 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder2.up.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 8, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder2.up.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder2.up.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder3.block.0.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (8, 1, 7, 7), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 8, 64, 64), bit_width=23.0, fixed-point format=Q9.13, scale=0.0001220703125, signed=True

Module: encoder3.block.0.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=23.0, fixed-point format=Q9.13, scale=0.0001220703125, signed=True

Module: encoder3.block.0.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder3.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder3.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder3.block.0.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 64, 64), bit_width=9.0, fixed-point format=Q3.5, scale=0.03125, signed=True
  * Quant Weight shape: (8, 1, 7, 7), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 8, 64, 64), bit_width=24.0, fixed-point format=Q9.14, scale=6.103515625e-05, signed=True

Module: encoder3.block.0.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=24.0, fixed-point format=Q9.14, scale=6.103515625e-05, signed=True

Module: encoder3.block.0.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder3.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder3.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder3.block.0.2 (QuantConv2d)
  * Quant Input shape: (1, 8, 64, 64), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 32, 64, 64), bit_width=21.0, fixed-point format=Q8.12, scale=0.000244140625, signed=True

Module: encoder3.block.0.3 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 64, 64])
  * Quant Output shape: (1, 32, 64, 64), bit_width=21.0, fixed-point format=Q8.12, scale=0.000244140625, signed=True

Module: encoder3.block.0.4 (QuantReLU)
  * Input shape: torch.Size([1, 32, 64, 64])
  * Quant Output shape: (1, 32, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder3.block.0.5 (QuantConv2d)
  * Quant Input shape: (1, 32, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 8, 64, 64), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder3.block.0.6 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder3.block.0.7 (QuantReLU)
  * Input shape: torch.Size([1, 8, 64, 64])
  * Quant Output shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder3.up.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (16, 8, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 64, 64), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder3.up.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder3.up.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder4.block.0.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (16, 1, 7, 7), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=23.0, fixed-point format=Q11.11, scale=0.00048828125, signed=True

Module: encoder4.block.0.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=23.0, fixed-point format=Q11.11, scale=0.00048828125, signed=True

Module: encoder4.block.0.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder4.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder4.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder4.block.0.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 32, 32), bit_width=9.0, fixed-point format=Q3.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 1, 7, 7), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder4.block.0.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder4.block.0.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder4.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder4.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder4.block.0.2 (QuantConv2d)
  * Quant Input shape: (1, 16, 32, 32), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (64, 16, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (64,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 64, 32, 32), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder4.block.0.3 (QuantIdentity)
  * Input shape: torch.Size([1, 64, 32, 32])
  * Quant Output shape: (1, 64, 32, 32), bit_width=22.0, fixed-point format=Q9.12, scale=0.000244140625, signed=True

Module: encoder4.block.0.4 (QuantReLU)
  * Input shape: torch.Size([1, 64, 32, 32])
  * Quant Output shape: (1, 64, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder4.block.0.5 (QuantConv2d)
  * Quant Input shape: (1, 64, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 64, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=23.0, fixed-point format=Q10.12, scale=0.000244140625, signed=True

Module: encoder4.block.0.6 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=23.0, fixed-point format=Q10.12, scale=0.000244140625, signed=True

Module: encoder4.block.0.7 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder4.up.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (16, 16, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: encoder4.up.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: encoder4.up.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 1, 9, 9), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 16, 16), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=24.0, fixed-point format=Q10.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=9.0, fixed-point format=Q3.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 1, 9, 9), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 16, 16), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.2 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=9.0, fixed-point format=Q3.5, scale=0.03125, signed=True
  * Quant Weight shape: (64, 16, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (64,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 64, 16, 16), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.3 (QuantIdentity)
  * Input shape: torch.Size([1, 64, 16, 16])
  * Quant Output shape: (1, 64, 16, 16), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.4 (QuantReLU)
  * Input shape: torch.Size([1, 64, 16, 16])
  * Quant Output shape: (1, 64, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.0.5 (QuantConv2d)
  * Quant Input shape: (1, 64, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 64, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 16, 16), bit_width=23.0, fixed-point format=Q9.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.6 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=23.0, fixed-point format=Q9.13, scale=0.0001220703125, signed=True

Module: encoder5.block.0.7 (QuantReLU)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.0.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 1, 9, 9), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 16, 16, 16), bit_width=24.0, fixed-point format=Q11.12, scale=0.000244140625, signed=True

Module: encoder5.block.1.0.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=24.0, fixed-point format=Q11.12, scale=0.000244140625, signed=True

Module: encoder5.block.1.0.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.0.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.1.fn.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=9.0, fixed-point format=Q3.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 1, 9, 9), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 16, 16), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: encoder5.block.1.1.fn.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: encoder5.block.1.1.fn.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: encoder5.block.1.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.1.input_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.2 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=9.0, fixed-point format=Q3.5, scale=0.03125, signed=True
  * Quant Weight shape: (64, 16, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (64,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 64, 16, 16), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: encoder5.block.1.3 (QuantIdentity)
  * Input shape: torch.Size([1, 64, 16, 16])
  * Quant Output shape: (1, 64, 16, 16), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: encoder5.block.1.4 (QuantReLU)
  * Input shape: torch.Size([1, 64, 16, 16])
  * Quant Output shape: (1, 64, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.block.1.5 (QuantConv2d)
  * Quant Input shape: (1, 64, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 64, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 16, 16, 16), bit_width=23.0, fixed-point format=Q10.12, scale=0.000244140625, signed=True

Module: encoder5.block.1.6 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=23.0, fixed-point format=Q10.12, scale=0.000244140625, signed=True

Module: encoder5.block.1.7 (QuantReLU)
  * Input shape: torch.Size([1, 16, 16, 16])
  * Quant Output shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: encoder5.up.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (24, 16, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (24,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 24, 16, 16), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: encoder5.up.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 24, 16, 16])
  * Quant Output shape: (1, 24, 16, 16), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: encoder5.up.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 24, 16, 16])
  * Quant Output shape: (1, 24, 16, 16), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up5.up.0 (QuantUpsamplingBilinear2d)
  * Input shape: torch.Size([1, 24, 16, 16])
  * Quant Output shape: (1, 24, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up5.up.1 (QuantConv2d)
  * Quant Input shape: (1, 24, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 24, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: Up5.up.2 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: Up5.up.3 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv5.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 32, 32), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (16, 16, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=26.0, fixed-point format=Q12.13, scale=0.0001220703125, signed=True

Module: Up_conv5.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=26.0, fixed-point format=Q12.13, scale=0.0001220703125, signed=True

Module: Up_conv5.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up_conv5.conv.3 (QuantConv2d)
  * Quant Input shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (64, 16, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (64,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 64, 32, 32), bit_width=21.0, fixed-point format=Q8.12, scale=0.000244140625, signed=True

Module: Up_conv5.conv.4 (QuantIdentity)
  * Input shape: torch.Size([1, 64, 32, 32])
  * Quant Output shape: (1, 64, 32, 32), bit_width=21.0, fixed-point format=Q8.12, scale=0.000244140625, signed=True

Module: Up_conv5.conv.5 (QuantReLU)
  * Input shape: torch.Size([1, 64, 32, 32])
  * Quant Output shape: (1, 64, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv5.conv.6 (QuantConv2d)
  * Quant Input shape: (1, 64, 32, 32), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (16, 64, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 16, 32, 32), bit_width=23.0, fixed-point format=Q10.12, scale=0.000244140625, signed=True

Module: Up_conv5.conv.7 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=23.0, fixed-point format=Q10.12, scale=0.000244140625, signed=True

Module: Up_conv5.conv.8 (QuantReLU)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 32, 32), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up4.up.0 (QuantUpsamplingBilinear2d)
  * Input shape: torch.Size([1, 16, 32, 32])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up4.up.1 (QuantConv2d)
  * Quant Input shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 16, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 64, 64), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: Up4.up.2 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=25.0, fixed-point format=Q11.13, scale=0.0001220703125, signed=True

Module: Up4.up.3 (QuantReLU)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv4.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 16, 64, 64), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (16, 16, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0009765625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 16, 64, 64), bit_width=26.0, fixed-point format=Q11.14, scale=6.103515625e-05, signed=True

Module: Up_conv4.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=26.0, fixed-point format=Q11.14, scale=6.103515625e-05, signed=True

Module: Up_conv4.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv4.conv.3 (QuantConv2d)
  * Quant Input shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (64, 16, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (64,), bit_width=16.0, fixed-point format=Q4.11, scale=0.00048828125, signed=True
  * Quant Output shape: (1, 64, 64, 64), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: Up_conv4.conv.4 (QuantIdentity)
  * Input shape: torch.Size([1, 64, 64, 64])
  * Quant Output shape: (1, 64, 64, 64), bit_width=21.0, fixed-point format=Q9.11, scale=0.00048828125, signed=True

Module: Up_conv4.conv.5 (QuantReLU)
  * Input shape: torch.Size([1, 64, 64, 64])
  * Quant Output shape: (1, 64, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up_conv4.conv.6 (QuantConv2d)
  * Quant Input shape: (1, 64, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (16, 64, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (16,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 16, 64, 64), bit_width=23.0, fixed-point format=Q9.13, scale=0.0001220703125, signed=True

Module: Up_conv4.conv.7 (QuantIdentity)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=23.0, fixed-point format=Q9.13, scale=0.0001220703125, signed=True

Module: Up_conv4.conv.8 (QuantReLU)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 64, 64), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up3.up.0 (QuantUpsamplingBilinear2d)
  * Input shape: torch.Size([1, 16, 64, 64])
  * Quant Output shape: (1, 16, 128, 128), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up3.up.1 (QuantConv2d)
  * Quant Input shape: (1, 16, 128, 128), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (8, 16, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.001953125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: Up3.up.2 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: Up3.up.3 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv3.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 128, 128), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 8, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0009765625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: Up_conv3.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: Up_conv3.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv3.conv.3 (QuantConv2d)
  * Quant Input shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q5.10, scale=0.0009765625, signed=True
  * Quant Output shape: (1, 32, 128, 128), bit_width=20.0, fixed-point format=Q9.10, scale=0.0009765625, signed=True

Module: Up_conv3.conv.4 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 128, 128])
  * Quant Output shape: (1, 32, 128, 128), bit_width=20.0, fixed-point format=Q9.10, scale=0.0009765625, signed=True

Module: Up_conv3.conv.5 (QuantReLU)
  * Input shape: torch.Size([1, 32, 128, 128])
  * Quant Output shape: (1, 32, 128, 128), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up_conv3.conv.6 (QuantConv2d)
  * Quant Input shape: (1, 32, 128, 128), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 8, 128, 128), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: Up_conv3.conv.7 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: Up_conv3.conv.8 (QuantReLU)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 128, 128), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True

Module: Up2.up.0 (QuantUpsamplingBilinear2d)
  * Input shape: torch.Size([1, 8, 128, 128])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True

Module: Up2.up.1 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True
  * Quant Weight shape: (8, 8, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.00390625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=24.0, fixed-point format=Q9.14, scale=6.103515625e-05, signed=True

Module: Up2.up.2 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=24.0, fixed-point format=Q9.14, scale=6.103515625e-05, signed=True

Module: Up2.up.3 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: x_quant (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q3.4, scale=0.0625, signed=True

Module: Up_conv2.conv.0 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=9.0, fixed-point format=Q4.4, scale=0.0625, signed=True
  * Quant Weight shape: (8, 8, 3, 3), bit_width=8.0, fixed-point format=Q0.7, scale=0.0009765625, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q1.14, scale=6.103515625e-05, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: Up_conv2.conv.1 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=25.0, fixed-point format=Q10.14, scale=6.103515625e-05, signed=True

Module: Up_conv2.conv.2 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Up_conv2.conv.3 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (32, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (32,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 32, 256, 256), bit_width=20.0, fixed-point format=Q7.12, scale=0.000244140625, signed=True

Module: Up_conv2.conv.4 (QuantIdentity)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=20.0, fixed-point format=Q7.12, scale=0.000244140625, signed=True

Module: Up_conv2.conv.5 (QuantReLU)
  * Input shape: torch.Size([1, 32, 256, 256])
  * Quant Output shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True

Module: Up_conv2.conv.6 (QuantConv2d)
  * Quant Input shape: (1, 32, 256, 256), bit_width=8.0, fixed-point format=Q1.6, scale=0.015625, signed=True
  * Quant Weight shape: (8, 32, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (8,), bit_width=16.0, fixed-point format=Q2.13, scale=0.0001220703125, signed=True
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: Up_conv2.conv.7 (QuantIdentity)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=22.0, fixed-point format=Q8.13, scale=0.0001220703125, signed=True

Module: Up_conv2.conv.8 (QuantReLU)
  * Input shape: torch.Size([1, 8, 256, 256])
  * Quant Output shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True

Module: Conv_1x1 (QuantConv2d)
  * Quant Input shape: (1, 8, 256, 256), bit_width=8.0, fixed-point format=Q2.5, scale=0.03125, signed=True
  * Quant Weight shape: (1, 8, 1, 1), bit_width=8.0, fixed-point format=Q0.7, scale=0.0078125, signed=True
  * Quant Bias shape: (1,), bit_width=16.0, fixed-point format=Q3.12, scale=0.000244140625, signed=True
  * Quant Output shape: (1, 1, 256, 256), bit_width=20.0, fixed-point format=Q7.12, scale=0.000244140625, signed=True

Module: output_quant (QuantIdentity)
  * Input shape: torch.Size([1, 1, 256, 256])
  * Quant Output shape: (1, 1, 256, 256), bit_width=8.0, fixed-point format=Q4.3, scale=0.125, signed=True
